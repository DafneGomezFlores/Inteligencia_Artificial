{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "JUEGO DE 4 EN RAYA AGENTE DE APRENDIZAJE POR REFUERZO UTILIZANDO Q-LEARNING\n",
        "\n"
      ],
      "metadata": {
        "id": "ZT6C5dpU10MI"
      },
      "id": "ZT6C5dpU10MI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estas se usan para manipulaci贸n de datos (NumPy), selecci贸n aleatoria y guardado/carga de objetos (pickle)"
      ],
      "metadata": {
        "id": "pShhlANI3j2Y"
      },
      "id": "pShhlANI3j2Y"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ab317a02",
      "metadata": {
        "id": "ab317a02"
      },
      "outputs": [],
      "source": [
        "#IMPORTACION DE LIBRERIAS\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define un tablero de 6 filas y 7 columnas, con la condici贸n de victoria de conectar 4 fichas."
      ],
      "metadata": {
        "id": "fA81g5RZ3o24"
      },
      "id": "fA81g5RZ3o24"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7ff2a365",
      "metadata": {
        "id": "7ff2a365"
      },
      "outputs": [],
      "source": [
        "#CONSTANTES\n",
        "ROWS = 6\n",
        "COLS = 7\n",
        "WIN_LENGTH = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3W0qI-m63wf5"
      },
      "id": "3W0qI-m63wf5"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1813b2aa",
      "metadata": {
        "id": "1813b2aa"
      },
      "outputs": [],
      "source": [
        "#Inicializa el tablero como una matriz de ceros (vac铆a).\n",
        "\n",
        "#El jugador actual es 1 (puede alternar con -1 m谩s adelante).\n",
        "class Connect4:\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((ROWS, COLS), dtype=int)\n",
        "        self.current_player = 1\n",
        "#Reinicia el tablero y el turno.\n",
        "\n",
        "#Devuelve el estado actual del tablero como una tupla.\n",
        "    def reset(self):\n",
        "        self.board[:] = 0\n",
        "        self.current_player = 1\n",
        "        return self.get_state()\n",
        "#Convierte el tablero 2D en una tupla 1D. Esto es 煤til para aprendizaje\n",
        "#autom谩tico (por ejemplo, como entrada para modelos o Q-tables).\n",
        "    def get_state(self):\n",
        "        return tuple(self.board.flatten())\n",
        "#Devuelve una lista de columnas donde todav铆a se pueden colocar fichas\n",
        "#(es decir, donde la fila superior est谩 vac铆a).\n",
        "    def available_actions(self):\n",
        "        return [c for c in range(COLS) if self.board[0][c] == 0]\n",
        "#Este m茅todo:\n",
        "\n",
        "#Verifica si la acci贸n es v谩lida.\n",
        "\n",
        "#Coloca la ficha del jugador actual en la columna elegida.\n",
        "\n",
        "#Revisa si el jugador gan贸.\n",
        "\n",
        "#Otorga una recompensa:\n",
        "\n",
        "#1 si gana,\n",
        "\n",
        "#0.5 si empata,\n",
        "\n",
        "#-10 si hace un movimiento inv谩lido,\n",
        "\n",
        "#0 si el juego contin煤a.\n",
        "\n",
        "#Alterna el turno si no ha terminado.\n",
        "\n",
        "#Devuelve el nuevo estado, la recompensa y si el juego ha terminado.\n",
        "    def step(self, action):\n",
        "        if action not in self.available_actions():\n",
        "            return self.get_state(), -10, True  # Movimiento inv谩lido\n",
        "\n",
        "        row = self.get_next_open_row(action)\n",
        "        self.board[row][action] = self.current_player\n",
        "        done = self.check_winner(self.current_player)\n",
        "\n",
        "        if done:\n",
        "            reward = 1\n",
        "        elif len(self.available_actions()) == 0:\n",
        "            reward = 0.5  # empate\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0\n",
        "            self.current_player *= -1\n",
        "\n",
        "        return self.get_state(), reward, done\n",
        "#Encuentra la fila m谩s baja disponible en una columna para colocar una ficha.\n",
        "    def get_next_open_row(self, col):\n",
        "        for r in range(ROWS - 1, -1, -1):\n",
        "            if self.board[r][col] == 0:\n",
        "                return r\n",
        "#Comprueba si hay 4 fichas consecutivas del jugador actual en:\n",
        "\n",
        "#Direcci贸n horizontal\n",
        "\n",
        "#Direcci贸n vertical\n",
        "\n",
        "#Diagonal descendente \n",
        "\n",
        "#Diagonal ascendente \n",
        "\n",
        "#Si encuentra alguna, devuelve True (hay ganador).\n",
        "    def check_winner(self, player):\n",
        "        for c in range(COLS - 3):\n",
        "            for r in range(ROWS):\n",
        "                if all(self.board[r, c + i] == player for i in range(WIN_LENGTH)):\n",
        "                    return True\n",
        "        for c in range(COLS):\n",
        "            for r in range(ROWS - 3):\n",
        "                if all(self.board[r + i, c] == player for i in range(WIN_LENGTH)):\n",
        "                    return True\n",
        "        for c in range(COLS - 3):\n",
        "            for r in range(ROWS - 3):\n",
        "                if all(self.board[r + i, c + i] == player for i in range(WIN_LENGTH)):\n",
        "                    return True\n",
        "        for c in range(COLS - 3):\n",
        "            for r in range(3, ROWS):\n",
        "                if all(self.board[r - i, c + i] == player for i in range(WIN_LENGTH)):\n",
        "                    return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wenbAbxo8zNT"
      },
      "id": "wenbAbxo8zNT"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8371e3a3",
      "metadata": {
        "id": "8371e3a3"
      },
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "  #INIALIZAMOS\n",
        "  #c贸mo debe lucir y qu茅 datos debe tener un objeto de tu clase justo despu茅s de ser creado.\n",
        "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.q_table = {}  # {(state, action): value}\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "#te permite consultar cu谩nto \"valor\" o \"recompensa esperada\"\n",
        "#ha aprendido el agente que se obtiene al realizar una acci贸n espec铆fica\n",
        "#cuando se encuentra en un estado determinado. Si esa combinaci贸n estado-acci贸n\n",
        "#es nueva para el agente, asume un valor inicial de 0.0. Esto es crucial para que\n",
        "#el agente pueda tomar decisiones bas谩ndose en lo que ha aprendido.\n",
        "    def get_q(self, state, action):\n",
        "        return self.q_table.get((state, action), 0.0)\n",
        "#Decide, bas谩ndose en el par谩metro epsilon, si tomar una acci贸n completamente aleatoria\n",
        "#para descubrir nuevas posibilidades (exploraci贸n) o si tomar la acci贸n que, seg煤n su\n",
        "#tabla Q, se espera que d茅 la mayor recompensa (explotaci贸n).\n",
        "#Este equilibrio es fundamental para que el agente aprenda de manera efectiva.\n",
        "    def choose_action(self, state, available_actions):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(available_actions)\n",
        "        qs = [self.get_q(state, a) for a in available_actions]\n",
        "        max_q = max(qs)\n",
        "        max_actions = [a for a, q in zip(available_actions, qs) if q == max_q]\n",
        "        return random.choice(max_actions)\n",
        "#Utiliza para refinar la estimaci贸n del valor de la acci贸n tomada en el estado anterior.\n",
        "#Al ajustar gradualmente los valores en la tabla Q bas谩ndose en estas experiencias,\n",
        "#el agente aprende cu谩les son las acciones m谩s valiosas en cada estado para maximizar\n",
        "#la recompensa a largo plazo.\n",
        "    def update(self, state, action, reward, next_state, next_actions):\n",
        "        max_q_next = max([self.get_q(next_state, a) for a in next_actions], default=0.0)\n",
        "        old_q = self.get_q(state, action)\n",
        "        self.q_table[(state, action)] = old_q + self.alpha * (reward + self.gamma * max_q_next - old_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "eebff0e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eebff0e5",
        "outputId": "4214b118-6803-4eb0-a8f9-92c50508c32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000\n",
            "Episode 2000\n",
            "Episode 3000\n",
            "Episode 4000\n",
            "Episode 5000\n",
            "Episode 6000\n",
            "Episode 7000\n",
            "Episode 8000\n",
            "Episode 9000\n",
            "Episode 10000\n",
            "Entrenamiento completado.\n"
          ]
        }
      ],
      "source": [
        "#ENTRENAMIENTO\n",
        "#es donde el agente de Q-Learning aprende a jugar Connect 4 a trav茅s de la interacci贸n\n",
        "#con el entorno del juego. Implementa el bucle principal de aprendizaje por refuerzo.\n",
        "env = Connect4()\n",
        "agent = QLearningAgent()\n",
        "#Definici贸n del n煤mero de episodios\n",
        "EPISODES = 10000\n",
        "#Bucle de entrenamiento\n",
        "for episode in range(EPISODES):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "#Bucle de juego dentro de cada episodio\n",
        "    while not done:\n",
        "        actions = env.available_actions()\n",
        "        action = agent.choose_action(state, actions)\n",
        "\n",
        "        next_state, reward, done = env.step(action)\n",
        "#Turno del oponente (Jugador -1, aqu铆 un oponente aleatorio)\n",
        "        if not done:\n",
        "            # El oponente hace una jugada aleatoria\n",
        "            opp_actions = env.available_actions()\n",
        "            if opp_actions:\n",
        "                opp_action = random.choice(opp_actions)\n",
        "                next_state, reward_opp, done = env.step(opp_action)\n",
        "                if done:\n",
        "                    reward = -1 if reward_opp == 1 else reward\n",
        "#Actualizaci贸n del agente\n",
        "        next_actions = env.available_actions()\n",
        "        agent.update(state, action, reward, next_state, next_actions)\n",
        "        state = next_state\n",
        "#Impresi贸n de progreso\n",
        "    if (episode + 1) % 1000 == 0:\n",
        "        print(f\"Episode {episode + 1}\")\n",
        "\n",
        "# Guardar la tabla Q\n",
        "with open(\"q_table_connect4.pkl\", \"wb\") as f:\n",
        "    pickle.dump(agent.q_table, f)\n",
        "\n",
        "print(\"Entrenamiento completado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6ad26fd8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ad26fd8",
        "outputId": "ae43ac48-2f42-4bda-fbe1-46d152e69a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "隆Bienvenido a 4 en Raya contra la IA!\n",
            "T煤 eres el jugador 2 (fichas con n煤mero 2)\n",
            "Para jugar, ingresa un n煤mero de columna del 0 al 6.\n",
            "\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "IA juega columna: 4\n",
            "[[0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "Tu turno (columna 0-6): 2\n",
            "[[0 0 2 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "IA juega columna: 3\n",
            "[[0 0 2 1 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "Tu turno (columna 0-6): 3\n",
            "[[0 0 2 1 1 0 0]\n",
            " [0 0 0 2 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "IA juega columna: 5\n",
            "[[0 0 2 1 1 1 0]\n",
            " [0 0 0 2 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "Tu turno (columna 0-6): 4\n",
            "[[0 0 2 1 1 1 0]\n",
            " [0 0 0 2 2 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "IA juega columna: 4\n",
            "[[0 0 2 1 1 1 0]\n",
            " [0 0 0 2 2 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "Tu turno (columna 0-6): 5\n",
            "[[0 0 2 1 1 1 0]\n",
            " [0 0 0 2 2 2 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "IA juega columna: 6\n",
            "[[0 0 2 1 1 1 1]\n",
            " [0 0 0 2 2 2 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "La IA gan贸 \n",
            "驴Jugar de nuevo? (s/n): 6\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "ROWS = 6\n",
        "COLS = 7\n",
        "#Re-define la clase Connect4: Vuelve a definir la clase Connect4. Aunque ya estaba definida\n",
        "#antes, se incluye aqu铆 nuevamente para asegurar que el c贸digo de juego tenga acceso a la\n",
        "#definici贸n del entorno, incluso si se ejecuta esta celda de forma independiente.\n",
        "class Connect4:\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((ROWS, COLS), dtype=int)\n",
        "        self.current_player = 1\n",
        "\n",
        "    def reset(self):\n",
        "        self.board[:] = 0\n",
        "        self.current_player = 1\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        return tuple(self.board.flatten())\n",
        "\n",
        "    def available_actions(self):\n",
        "        return [c for c in range(COLS) if self.board[0][c] == 0]\n",
        "\n",
        "    def step(self, action):\n",
        "        if action not in self.available_actions():\n",
        "            return self.get_state(), -10, True  # Movimiento inv谩lido\n",
        "\n",
        "        row = self.get_next_open_row(action)\n",
        "        self.board[row][action] = self.current_player\n",
        "        done = self.check_winner(self.current_player)\n",
        "\n",
        "        if done:\n",
        "            reward = 1\n",
        "        elif len(self.available_actions()) == 0:\n",
        "            reward = 0.5\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0\n",
        "            self.current_player *= -1\n",
        "\n",
        "        return self.get_state(), reward, done\n",
        "\n",
        "    def get_next_open_row(self, col):\n",
        "        for r in range(ROWS - 1, -1, -1):\n",
        "            if self.board[r][col] == 0:\n",
        "                return r\n",
        "\n",
        "    def check_winner(self, player):\n",
        "        for c in range(COLS - 3):\n",
        "            for r in range(ROWS):\n",
        "                if all(self.board[r, c + i] == player for i in range(4)):\n",
        "                    return True\n",
        "        for c in range(COLS):\n",
        "            for r in range(ROWS - 3):\n",
        "                if all(self.board[r + i, c] == player for i in range(4)):\n",
        "                    return True\n",
        "        for c in range(COLS - 3):\n",
        "            for r in range(ROWS - 3):\n",
        "                if all(self.board[r + i, c + i] == player for i in range(4)):\n",
        "                    return True\n",
        "        for c in range(COLS - 3):\n",
        "            for r in range(3, ROWS):\n",
        "                if all(self.board[r - i, c + i] == player for i in range(4)):\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    def print_board(self):\n",
        "        display = self.board.copy()\n",
        "        display[display == -1] = 2  # Para mostrar el jugador 2 como \"2\"\n",
        "        print(np.flip(display, 0))  # Mostrar con el fondo abajo\n",
        "\n",
        "# Cargar tabla Q entrada\n",
        "with open(\"q_table_connect4.pkl\", \"rb\") as f:\n",
        "    q_table = pickle.load(f)\n",
        "\n",
        "def get_q(q_table, state, action):\n",
        "    return q_table.get((state, action), 0.0)\n",
        "\n",
        "def agent_choose_action(state, available_actions):\n",
        "    qs = [get_q(q_table, state, a) for a in available_actions]\n",
        "    max_q = max(qs)\n",
        "    best_actions = [a for a, q in zip(available_actions, qs) if q == max_q]\n",
        "    return np.random.choice(best_actions)\n",
        "\n",
        "# Jugar contra el agente\n",
        "env = Connect4()\n",
        "\n",
        "print(\"隆Bienvenido a 4 en Raya contra la IA!\\nT煤 eres el jugador 2 (fichas con n煤mero 2)\")\n",
        "print(\"Para jugar, ingresa un n煤mero de columna del 0 al 6.\\n\")\n",
        "#Bucle principal de juego\n",
        "while True:\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    human_player = -1\n",
        "\n",
        "    while not done:\n",
        "        env.print_board()\n",
        "\n",
        "        if env.current_player == human_player:\n",
        "            try:\n",
        "                col = int(input(\"Tu turno (columna 0-6): \"))\n",
        "                if col not in env.available_actions():\n",
        "                    print(\"Columna inv谩lida o llena. Intenta otra.\")\n",
        "                    continue\n",
        "            except ValueError:\n",
        "                print(\"Entrada inv谩lida.\")\n",
        "                continue\n",
        "        else:\n",
        "            col = agent_choose_action(state, env.available_actions())\n",
        "            print(f\"IA juega columna: {col}\")\n",
        "\n",
        "        next_state, reward, done = env.step(col)\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            env.print_board()\n",
        "            if reward == 1 and env.current_player == human_player:\n",
        "                print(\"隆Ganaste!\")\n",
        "            elif reward == 1:\n",
        "                print(\"La IA gan贸 \")\n",
        "            elif reward == 0.5:\n",
        "                print(\"隆Empate!\")\n",
        "            elif reward == -10:\n",
        "                print(\"Movimiento inv谩lido. Perdiste.\")\n",
        "            break\n",
        "\n",
        "    again = input(\"驴Jugar de nuevo? (s/n): \").strip().lower()\n",
        "    if again != 's':\n",
        "        break\n",
        "#Este bloque te permite interactuar directamente con el agente de Q-Learning que entrenaste,\n",
        "# para ver qu茅 tan bien juega contra un oponente humano"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONCLUSION  \n",
        "entrenamiento e interacci贸n con un agente de aprendizaje por refuerzo para jugar al juego Connect 4  \n",
        "En esencia, estamos haciendo que un programa de computadora aprenda a jugar un juego complejo (Connect 4) simplemente jugando muchas veces y ajustando su estrategia bas谩ndose en si sus acciones resultaron en una victoria, una derrota o si le acercaron a un buen estado, y luego te permitimos desafiar a este programa que ha \"aprendido\".  \n",
        "-- Definici贸n del Entorno (Connect4 class): Creamos las reglas del juego Connect 4. Esto incluye c贸mo es el tablero, c贸mo se hacen los movimientos, c贸mo se detecta un ganador o un empate, y c贸mo se representa el estado del juego. Esta clase act煤a como el \"mundo\" con el que el agente interactuar谩.  \n",
        "-- Creaci贸n y Entrenamiento del Agente (QLearningAgent class y el bucle de entrenamiento):\n",
        "Dise帽amos un agente que utiliza el algoritmo Q-Learning. Este agente tiene una \"tabla Q\" donde almacenar谩 su conocimiento sobre qu茅 tan buenas son las acciones en diferentes estados del juego.\n",
        "Sometemos al agente a un proceso de entrenamiento masivo (muchos episodios). En cada episodio, el agente juega una partida, toma decisiones (a veces explorando aleatoriamente, a veces explotando lo aprendido), recibe recompensas del entorno por sus acciones, y utiliza esas recompensas para actualizar y mejorar los valores en su tabla Q. Durante el entrenamiento, el agente aprende a asociar estados del tablero con las acciones que probablemente lo lleven a ganar (o evitar perder).\n",
        "Guardamos el conocimiento aprendido por el agente (su tabla Q) en un archivo para poder usarlo m谩s tarde.  \n",
        "Interacci贸n Humano-Agente (el 煤ltimo bloque de c贸digo):\n",
        "\n",
        "Cargamos la tabla Q que el agente entrenado gener贸.\n",
        "Creamos una interfaz donde un jugador humano puede jugar partidas de Connect 4 directamente contra el agente entrenado.\n",
        "El agente utiliza la tabla Q cargada para tomar sus decisiones de juego (siempre eligiendo la mejor opci贸n conocida, sin exploraci贸n), mientras que el humano ingresa sus movimientos.  \n",
        "Interacci贸n Humano-Agente (el 煤ltimo bloque de c贸digo):\n",
        "-- Cargamos la tabla Q que el agente entrenado gener贸.\n",
        "Creamos una interfaz donde un jugador humano puede jugar partidas de Connect 4 directamente contra el agente entrenado.\n",
        "El agente utiliza la tabla Q cargada para tomar sus decisiones de juego (siempre eligiendo la mejor opci贸n conocida, sin exploraci贸n), mientras que el humano ingresa sus movimientos.\n"
      ],
      "metadata": {
        "id": "3pPDVy6d0h5J"
      },
      "id": "3pPDVy6d0h5J"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}