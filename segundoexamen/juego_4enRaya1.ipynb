{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "JUEGO DE 4 EN RAYA AGENTE DE APRENDIZAJE POR REFUERZO UTILIZANDO Q-LEARNING\n",
        "\n"
      ],
      "metadata": {
        "id": "ZT6C5dpU10MI"
      },
      "id": "ZT6C5dpU10MI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estas se usan para manipulación de datos (NumPy), selección aleatoria y guardado/carga de objetos (pickle)"
      ],
      "metadata": {
        "id": "pShhlANI3j2Y"
      },
      "id": "pShhlANI3j2Y"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ab317a02",
      "metadata": {
        "id": "ab317a02"
      },
      "outputs": [],
      "source": [
        "#IMPORTACION DE LIBRERIAS\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define un tablero de 6 filas y 7 columnas, con la condición de victoria de conectar 4 fichas."
      ],
      "metadata": {
        "id": "fA81g5RZ3o24"
      },
      "id": "fA81g5RZ3o24"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7ff2a365",
      "metadata": {
        "id": "7ff2a365"
      },
      "outputs": [],
      "source": [
        "#CONSTANTES\n",
        "ROWS = 6\n",
        "COLS = 7\n",
        "WIN_LENGTH = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3W0qI-m63wf5"
      },
      "id": "3W0qI-m63wf5"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1813b2aa",
      "metadata": {
        "id": "1813b2aa"
      },
      "outputs": [],
      "source": [
        "#Inicializa el tablero como una matriz de ceros (vacía).\n",
        "\n",
        "#El jugador actual es 1 (puede alternar con -1 más adelante).\n",
        "class Connect4:\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((ROWS, COLS), dtype=int)\n",
        "        self.current_player = 1\n",
        "#Reinicia el tablero y el turno.\n",
        "\n",
        "#Devuelve el estado actual del tablero como una tupla.\n",
        "    def reset(self):\n",
        "        self.board[:] = 0\n",
        "        self.current_player = 1\n",
        "        return self.get_state()\n",
        "#Convierte el tablero 2D en una tupla 1D. Esto es útil para aprendizaje\n",
        "#automático (por ejemplo, como entrada para modelos o Q-tables).\n",
        "    def get_state(self):\n",
        "        return tuple(self.board.flatten())\n",
        "#Devuelve una lista de columnas donde todavía se pueden colocar fichas\n",
        "#(es decir, donde la fila superior está vacía).\n",
        "    def available_actions(self):\n",
        "        return [c for c in range(COLS) if self.board[0][c] == 0]\n",
        "#Este método:\n",
        "\n",
        "#Verifica si la acción es válida.\n",
        "\n",
        "#Coloca la ficha del jugador actual en la columna elegida.\n",
        "\n",
        "#Revisa si el jugador ganó.\n",
        "\n",
        "#Otorga una recompensa:\n",
        "\n",
        "#1 si gana,\n",
        "\n",
        "#0.5 si empata,\n",
        "\n",
        "#-10 si hace un movimiento inválido,\n",
        "\n",
        "#0 si el juego continúa.\n",
        "\n",
        "#Alterna el turno si no ha terminado.\n",
        "\n",
        "#Devuelve el nuevo estado, la recompensa y si el juego ha terminado.\n",
        "    def step(self, action):\n",
        "        if action not in self.available_actions():\n",
        "            return self.get_state(), -10, True  # Movimiento inválido\n",
        "\n",
        "        row = self.get_next_open_row(action)\n",
        "        self.board[row][action] = self.current_player\n",
        "        done = self.check_winner(self.current_player)\n",
        "\n",
        "        if done:\n",
        "            reward = 1\n",
        "        elif len(self.available_actions()) == 0:\n",
        "            reward = 0.5  # empate\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0\n",
        "            self.current_player *= -1\n",
        "\n",
        "        return self.get_state(), reward, done\n",
        "#Encuentra la fila más baja disponible en una columna para colocar una ficha.\n",
        "    def get_next_open_row(self, col):\n",
        "        for r in range(ROWS - 1, -1, -1):\n",
        "            if self.board[r][col] == 0:\n",
        "                return r\n",
        "#Comprueba si hay 4 fichas consecutivas del jugador actual en:\n",
        "\n",
        "#Dirección horizontal\n",
        "\n",
        "#Dirección vertical\n",
        "\n",
        "#Diagonal descendente ↘\n",
        "\n",
        "#Diagonal ascendente ↗\n",
        "\n",
        "#Si encuentra alguna, devuelve True (hay ganador).\n",
        "    def check_winner(self, player):\n",
        "        for c in range(COLS - 3):\n",
        "            for r in range(ROWS):\n",
        "                if all(self.board[r, c + i] == player for i in range(WIN_LENGTH)):\n",
        "                    return True\n",
        "        for c in range(COLS):\n",
        "            for r in range(ROWS - 3):\n",
        "                if all(self.board[r + i, c] == player for i in range(WIN_LENGTH)):\n",
        "                    return True\n",
        "        for c in range(COLS - 3):\n",
        "            for r in range(ROWS - 3):\n",
        "                if all(self.board[r + i, c + i] == player for i in range(WIN_LENGTH)):\n",
        "                    return True\n",
        "        for c in range(COLS - 3):\n",
        "            for r in range(3, ROWS):\n",
        "                if all(self.board[r - i, c + i] == player for i in range(WIN_LENGTH)):\n",
        "                    return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wenbAbxo8zNT"
      },
      "id": "wenbAbxo8zNT"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8371e3a3",
      "metadata": {
        "id": "8371e3a3"
      },
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "  #INIALIZAMOS\n",
        "  #cómo debe lucir y qué datos debe tener un objeto de tu clase justo después de ser creado.\n",
        "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.q_table = {}  # {(state, action): value}\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "#te permite consultar cuánto \"valor\" o \"recompensa esperada\"\n",
        "#ha aprendido el agente que se obtiene al realizar una acción específica\n",
        "#cuando se encuentra en un estado determinado. Si esa combinación estado-acción\n",
        "#es nueva para el agente, asume un valor inicial de 0.0. Esto es crucial para que\n",
        "#el agente pueda tomar decisiones basándose en lo que ha aprendido.\n",
        "    def get_q(self, state, action):\n",
        "        return self.q_table.get((state, action), 0.0)\n",
        "#Decide, basándose en el parámetro epsilon, si tomar una acción completamente aleatoria\n",
        "#para descubrir nuevas posibilidades (exploración) o si tomar la acción que, según su\n",
        "#tabla Q, se espera que dé la mayor recompensa (explotación).\n",
        "#Este equilibrio es fundamental para que el agente aprenda de manera efectiva.\n",
        "    def choose_action(self, state, available_actions):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.choice(available_actions)\n",
        "        qs = [self.get_q(state, a) for a in available_actions]\n",
        "        max_q = max(qs)\n",
        "        max_actions = [a for a, q in zip(available_actions, qs) if q == max_q]\n",
        "        return random.choice(max_actions)\n",
        "#Utiliza para refinar la estimación del valor de la acción tomada en el estado anterior.\n",
        "#Al ajustar gradualmente los valores en la tabla Q basándose en estas experiencias,\n",
        "#el agente aprende cuáles son las acciones más valiosas en cada estado para maximizar\n",
        "#la recompensa a largo plazo.\n",
        "    def update(self, state, action, reward, next_state, next_actions):\n",
        "        max_q_next = max([self.get_q(next_state, a) for a in next_actions], default=0.0)\n",
        "        old_q = self.get_q(state, action)\n",
        "        self.q_table[(state, action)] = old_q + self.alpha * (reward + self.gamma * max_q_next - old_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "eebff0e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eebff0e5",
        "outputId": "4214b118-6803-4eb0-a8f9-92c50508c32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1000\n",
            "Episode 2000\n",
            "Episode 3000\n",
            "Episode 4000\n",
            "Episode 5000\n",
            "Episode 6000\n",
            "Episode 7000\n",
            "Episode 8000\n",
            "Episode 9000\n",
            "Episode 10000\n",
            "Entrenamiento completado.\n"
          ]
        }
      ],
      "source": [
        "#ENTRENAMIENTO\n",
        "#es donde el agente de Q-Learning aprende a jugar Connect 4 a través de la interacción\n",
        "#con el entorno del juego. Implementa el bucle principal de aprendizaje por refuerzo.\n",
        "env = Connect4()\n",
        "agent = QLearningAgent()\n",
        "#Definición del número de episodios\n",
        "EPISODES = 10000\n",
        "#Bucle de entrenamiento\n",
        "for episode in range(EPISODES):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "#Bucle de juego dentro de cada episodio\n",
        "    while not done:\n",
        "        actions = env.available_actions()\n",
        "        action = agent.choose_action(state, actions)\n",
        "\n",
        "        next_state, reward, done = env.step(action)\n",
        "#Turno del oponente (Jugador -1, aquí un oponente aleatorio)\n",
        "        if not done:\n",
        "            # El oponente hace una jugada aleatoria\n",
        "            opp_actions = env.available_actions()\n",
        "            if opp_actions:\n",
        "                opp_action = random.choice(opp_actions)\n",
        "                next_state, reward_opp, done = env.step(opp_action)\n",
        "                if done:\n",
        "                    reward = -1 if reward_opp == 1 else reward\n",
        "#Actualización del agente\n",
        "        next_actions = env.available_actions()\n",
        "        agent.update(state, action, reward, next_state, next_actions)\n",
        "        state = next_state\n",
        "#Impresión de progreso\n",
        "    if (episode + 1) % 1000 == 0:\n",
        "        print(f\"Episode {episode + 1}\")\n",
        "\n",
        "# Guardar la tabla Q\n",
        "with open(\"q_table_connect4.pkl\", \"wb\") as f:\n",
        "    pickle.dump(agent.q_table, f)\n",
        "\n",
        "print(\"Entrenamiento completado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6ad26fd8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ad26fd8",
        "outputId": "ae43ac48-2f42-4bda-fbe1-46d152e69a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Bienvenido a 4 en Raya contra la IA!\n",
            "Tú eres el jugador 2 (fichas con número 2)\n",
            "Para jugar, ingresa un número de columna del 0 al 6.\n",
            "\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "IA juega columna: 4\n",
            "[[0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "Tu turno (columna 0-6): 2\n",
            "[[0 0 2 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "IA juega columna: 3\n",
            "[[0 0 2 1 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "Tu turno (columna 0-6): 3\n",
            "[[0 0 2 1 1 0 0]\n",
            " [0 0 0 2 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "IA juega columna: 5\n",
            "[[0 0 2 1 1 1 0]\n",
            " [0 0 0 2 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "Tu turno (columna 0-6): 4\n",
            "[[0 0 2 1 1 1 0]\n",
            " [0 0 0 2 2 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "IA juega columna: 4\n",
            "[[0 0 2 1 1 1 0]\n",
            " [0 0 0 2 2 0 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "Tu turno (columna 0-6): 5\n",
            "[[0 0 2 1 1 1 0]\n",
            " [0 0 0 2 2 2 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "IA juega columna: 6\n",
            "[[0 0 2 1 1 1 1]\n",
            " [0 0 0 2 2 2 0]\n",
            " [0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "La IA ganó 😞\n",
            "¿Jugar de nuevo? (s/n): 6\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "ROWS = 6\n",
        "COLS = 7\n",
        "#Re-define la clase Connect4: Vuelve a definir la clase Connect4. Aunque ya estaba definida\n",
        "#antes, se incluye aquí nuevamente para asegurar que el código de juego tenga acceso a la\n",
        "#definición del entorno, incluso si se ejecuta esta celda de forma independiente.\n",
        "class Connect4:\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((ROWS, COLS), dtype=int)\n",
        "        self.current_player = 1\n",
        "\n",
        "    def reset(self):\n",
        "        self.board[:] = 0\n",
        "        self.current_player = 1\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        return tuple(self.board.flatten())\n",
        "\n",
        "    def available_actions(self):\n",
        "        return [c for c in range(COLS) if self.board[0][c] == 0]\n",
        "\n",
        "    def step(self, action):\n",
        "        if action not in self.available_actions():\n",
        "            return self.get_state(), -10, True  # Movimiento inválido\n",
        "\n",
        "        row = self.get_next_open_row(action)\n",
        "        self.board[row][action] = self.current_player\n",
        "        done = self.check_winner(self.current_player)\n",
        "\n",
        "        if done:\n",
        "            reward = 1\n",
        "        elif len(self.available_actions()) == 0:\n",
        "            reward = 0.5\n",
        "            done = True\n",
        "        else:\n",
        "            reward = 0\n",
        "            self.current_player *= -1\n",
        "\n",
        "        return self.get_state(), reward, done\n",
        "\n",
        "    def get_next_open_row(self, col):\n",
        "        for r in range(ROWS - 1, -1, -1):\n",
        "            if self.board[r][col] == 0:\n",
        "                return r\n",
        "\n",
        "    def check_winner(self, player):\n",
        "        for c in range(COLS - 3):\n",
        "            for r in range(ROWS):\n",
        "                if all(self.board[r, c + i] == player for i in range(4)):\n",
        "                    return True\n",
        "        for c in range(COLS):\n",
        "            for r in range(ROWS - 3):\n",
        "                if all(self.board[r + i, c] == player for i in range(4)):\n",
        "                    return True\n",
        "        for c in range(COLS - 3):\n",
        "            for r in range(ROWS - 3):\n",
        "                if all(self.board[r + i, c + i] == player for i in range(4)):\n",
        "                    return True\n",
        "        for c in range(COLS - 3):\n",
        "            for r in range(3, ROWS):\n",
        "                if all(self.board[r - i, c + i] == player for i in range(4)):\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    def print_board(self):\n",
        "        display = self.board.copy()\n",
        "        display[display == -1] = 2  # Para mostrar el jugador 2 como \"2\"\n",
        "        print(np.flip(display, 0))  # Mostrar con el fondo abajo\n",
        "\n",
        "# Cargar tabla Q entrada\n",
        "with open(\"q_table_connect4.pkl\", \"rb\") as f:\n",
        "    q_table = pickle.load(f)\n",
        "\n",
        "def get_q(q_table, state, action):\n",
        "    return q_table.get((state, action), 0.0)\n",
        "\n",
        "def agent_choose_action(state, available_actions):\n",
        "    qs = [get_q(q_table, state, a) for a in available_actions]\n",
        "    max_q = max(qs)\n",
        "    best_actions = [a for a, q in zip(available_actions, qs) if q == max_q]\n",
        "    return np.random.choice(best_actions)\n",
        "\n",
        "# Jugar contra el agente\n",
        "env = Connect4()\n",
        "\n",
        "print(\"¡Bienvenido a 4 en Raya contra la IA!\\nTú eres el jugador 2 (fichas con número 2)\")\n",
        "print(\"Para jugar, ingresa un número de columna del 0 al 6.\\n\")\n",
        "#Bucle principal de juego\n",
        "while True:\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    human_player = -1\n",
        "\n",
        "    while not done:\n",
        "        env.print_board()\n",
        "\n",
        "        if env.current_player == human_player:\n",
        "            try:\n",
        "                col = int(input(\"Tu turno (columna 0-6): \"))\n",
        "                if col not in env.available_actions():\n",
        "                    print(\"Columna inválida o llena. Intenta otra.\")\n",
        "                    continue\n",
        "            except ValueError:\n",
        "                print(\"Entrada inválida.\")\n",
        "                continue\n",
        "        else:\n",
        "            col = agent_choose_action(state, env.available_actions())\n",
        "            print(f\"IA juega columna: {col}\")\n",
        "\n",
        "        next_state, reward, done = env.step(col)\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            env.print_board()\n",
        "            if reward == 1 and env.current_player == human_player:\n",
        "                print(\"¡Ganaste!\")\n",
        "            elif reward == 1:\n",
        "                print(\"La IA ganó 😞\")\n",
        "            elif reward == 0.5:\n",
        "                print(\"¡Empate!\")\n",
        "            elif reward == -10:\n",
        "                print(\"Movimiento inválido. Perdiste.\")\n",
        "            break\n",
        "\n",
        "    again = input(\"¿Jugar de nuevo? (s/n): \").strip().lower()\n",
        "    if again != 's':\n",
        "        break\n",
        "#Este bloque te permite interactuar directamente con el agente de Q-Learning que entrenaste,\n",
        "# para ver qué tan bien juega contra un oponente humano"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONCLUSION  \n",
        "entrenamiento e interacción con un agente de aprendizaje por refuerzo para jugar al juego Connect 4  \n",
        "En esencia, estamos haciendo que un programa de computadora aprenda a jugar un juego complejo (Connect 4) simplemente jugando muchas veces y ajustando su estrategia basándose en si sus acciones resultaron en una victoria, una derrota o si le acercaron a un buen estado, y luego te permitimos desafiar a este programa que ha \"aprendido\".  \n",
        "-- Definición del Entorno (Connect4 class): Creamos las reglas del juego Connect 4. Esto incluye cómo es el tablero, cómo se hacen los movimientos, cómo se detecta un ganador o un empate, y cómo se representa el estado del juego. Esta clase actúa como el \"mundo\" con el que el agente interactuará.  \n",
        "-- Creación y Entrenamiento del Agente (QLearningAgent class y el bucle de entrenamiento):\n",
        "Diseñamos un agente que utiliza el algoritmo Q-Learning. Este agente tiene una \"tabla Q\" donde almacenará su conocimiento sobre qué tan buenas son las acciones en diferentes estados del juego.\n",
        "Sometemos al agente a un proceso de entrenamiento masivo (muchos episodios). En cada episodio, el agente juega una partida, toma decisiones (a veces explorando aleatoriamente, a veces explotando lo aprendido), recibe recompensas del entorno por sus acciones, y utiliza esas recompensas para actualizar y mejorar los valores en su tabla Q. Durante el entrenamiento, el agente aprende a asociar estados del tablero con las acciones que probablemente lo lleven a ganar (o evitar perder).\n",
        "Guardamos el conocimiento aprendido por el agente (su tabla Q) en un archivo para poder usarlo más tarde.  \n",
        "Interacción Humano-Agente (el último bloque de código):\n",
        "\n",
        "Cargamos la tabla Q que el agente entrenado generó.\n",
        "Creamos una interfaz donde un jugador humano puede jugar partidas de Connect 4 directamente contra el agente entrenado.\n",
        "El agente utiliza la tabla Q cargada para tomar sus decisiones de juego (siempre eligiendo la mejor opción conocida, sin exploración), mientras que el humano ingresa sus movimientos.  \n",
        "Interacción Humano-Agente (el último bloque de código):\n",
        "-- Cargamos la tabla Q que el agente entrenado generó.\n",
        "Creamos una interfaz donde un jugador humano puede jugar partidas de Connect 4 directamente contra el agente entrenado.\n",
        "El agente utiliza la tabla Q cargada para tomar sus decisiones de juego (siempre eligiendo la mejor opción conocida, sin exploración), mientras que el humano ingresa sus movimientos.\n"
      ],
      "metadata": {
        "id": "3pPDVy6d0h5J"
      },
      "id": "3pPDVy6d0h5J"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}